11 min read
·
28 mai 2024
… Dans le domaine en évolution rapide de l'intelligence artificielle, la capacité à résumer efficacement le contenu vidéo en utilisant des modèles avancés comme GPT-4o représente une avancée technologique significative. La dernière itération d'OpenAI, GPT-4o, introduite en mai 2024, intègre des capacités multimodales qui traitent le texte, les images et l'audio dans un cadre unique. Cette intégration facilite une compréhension plus holistique du contenu multimédia, rendant GPT-4o particulièrement adapté aux tâches telles que la summarisation de vidéos.

La summarisation de vidéos est de plus en plus critique à mesure que le volume de contenu vidéo numérique croît exponentiellement. Les entreprises, les éducateurs et les créateurs de contenu sont parmi ceux qui peuvent bénéficier considérablement des systèmes automatisés capables de condenser de longues vidéos en résumés concis et informatifs. Le modèle GPT-4o, avec ses capacités de traitement améliorées, offre une solution prometteuse à ce défi. Il comprend non seulement les cadres individuels, mais interprète également le contexte audio et textuel, fournissant des résumés à la fois complets et contextuellement pertinents.

L'introduction de GPT-4o a été marquée par des améliorations significatives en termes de vitesse et d'efficacité, comme détaillé dans le blog GPT-4o Vision Use Cases par Roboflow. Ces améliorations rendent GPT-4o adapté aux applications en temps réel, y compris la summarisation vidéo en direct, ce qui pourrait révolutionner notre interaction avec le contenu vidéo dans divers secteurs.

## Capacités multimodales de GPT-4o dans la summarisation vidéo

### Aperçu de l'intégration multimodale de GPT-4o
GPT-4o, ou GPT-4 Omni, représente une avancée significative dans la technologie de l'IA, conçue pour traiter et intégrer plusieurs formes de données d'entrée — texte, audio et visuel — de manière transparente. Cette capacité est cruciale pour des tâches telles que la summarisation de vidéos, où comprendre et interpréter simultanément divers types de données peut améliorer considérablement la qualité et la précision des résumés produits. L'architecture de GPT-4o lui permet de gérer ces entrées diversifiées en temps réel, en faisant un outil idéal pour des applications dynamiques et complexes telles que l'analyse de contenu vidéo en direct.

### Traitement et réponse en temps réel
L'une des caractéristiques remarquables de GPT-4o est sa capacité à traiter les entrées et à fournir des sorties avec une latence minimale. Dans le contexte de la summarisation de vidéos, cela signifie que GPT-4o peut analyser des flux vidéo en temps réel, interprétant à la fois le contenu visuel et tout audio accompagnant (comme les dialogues ou les sons de fond) pour générer des résumés concis et pertinents. Le temps de réponse du modèle, qui peut être aussi rapide que 232 millisecondes pour les entrées audio, garantit que les résumés sont générés presque instantanément, suivant les flux en direct si nécessaire.

### Compréhension des langues et des audios
Le support étendu de la langue par GPT-4o, qui comprend plus de 50 langues, est particulièrement bénéfique pour la summarisation de vidéos dans un contexte mondial. Cette fonctionnalité permet à GPT-4o non seulement de générer des résumés dans plusieurs langues mais aussi de comprendre et d'interpréter efficacement les entrées audio non anglophones. Qu'il s'agisse d'un clip d'actualités en espagnol ou d'un documentaire en hindi, GPT-4o peut fournir des résumés précis culturellement et contextuellement pertinents. Cette capacité multilingue assure une accessibilité et une utilisabilité plus larges de la technologie de summarisation dans différentes régions et démographies.

### Application dans divers domaines
L'application de GPT-4o dans la summarisation de vidéos s'étend à divers domaines, de contenu éducatif et réunions d'entreprise aux diffusions médiatiques et à la santé. Dans l'éducation, par exemple, GPT-4o peut être utilisé pour résumer des vidéos pédagogiques, rendant les informations clés plus accessibles aux étudiants et aux éducateurs. Dans le domaine de la santé, les résumés vidéo peuvent aider les professionnels à comprendre rapidement le contenu des vidéos de formation médicale ou des interactions avec les patients sans avoir à regarder toute la séquence. Cette polyvalence souligne le potentiel de GPT-4o à transformer la manière dont les informations sont consommées et utilisées dans les milieux professionnels.

### Améliorer l'accessibilité et l'inclusivité
La capacité de GPT-4o à intégrer et à interpréter des données textuelles, audio et visuelles en temps réel joue également un rôle crucial dans l'amélioration de l'accessibilité. Pour les personnes handicapées, telles que les personnes sourdes ou malentendantes, GPT-4o peut fournir une summarisation vidéo en temps réel avec des sorties textuelles. Cette fonctionnalité rend non seulement le contenu plus accessible, mais garantit également l'inclusivité, permettant à un public plus large de bénéficier des matériaux vidéo. Les fonctionnalités de transcription et de traduction en temps réel augmentent encore cette accessibilité en fournissant des sous-titres et des traductions en direct, élargissant la portée et la compréhension du contenu vidéo parmi les locuteurs de différentes langues.

## Techniques de summarisation vidéo dans GPT-4o

### Extraction de caractéristiques des cadres vidéo
GPT-4o utilise des techniques avancées d'extraction de caractéristiques pour analyser efficacement le contenu vidéo. Ce processus implique l'identification des cadres et segments clés dans une vidéo qui sont cruciaux pour générer un résumé concis. En traitant les cadres à différents intervalles, GPT-4o peut capturer à la fois le récit global et les détails significatifs, assurant que le résumé est représentatif de l'ensemble de la vidéo.

### Intégration de données multimodales
L'une des capacités remarquables de GPT-4o dans la summarisation vidéo est sa capacité à intégrer des entrées de données multimodales. Cela signifie que le modèle ne traite pas seulement les données visuelles, mais prend également en compte les indices audio et les métadonnées textuelles (comme les sous-titres ou les descriptions) pour améliorer la qualité du résumé. Par exemple, les changements de ton ou de musique dans l'audio peuvent indiquer un changement d'humeur dans la scène, ce qui est crucial pour un résumé précis. Cette approche holistique permet à GPT-4o de générer des résumés qui sont non seulement précis mais aussi riches en contexte.

### Cohérence sémantique et cohérence
Assurer la cohérence sémantique et la cohérence dans le résumé vidéo généré est un point focal critique pour GPT-4o. Le modèle utilise des techniques avancées de traitement du langage naturel pour maintenir un flux logique dans le récit qu'il crée. Cela implique l'utilisation de mécanismes d'attention qui aident le modèle à suivre le développement de l'histoire tout au long de la vidéo, garantissant que le résumé est logiquement structuré et exempt d'informations redondantes ou non pertinentes. Cette capacité est particulièrement importante dans les contextes éducatifs où la clarté des informations peut améliorer les résultats d'apprentissage.

### Mise en œuvre
Commençons par installer les bibliothèques requises.
```python
%pip install opencv-python --quiet 
%pip install moviepy --quiet
```
Séparons maintenant les cadres et l'audio d'une vidéo donnée.
```python
import cv2 
from moviepy.editor import VideoFileClip 
import time 
import base64

# Nous utiliserons la vidéo de récapitulation de la conférence OpenAI DevDay. Vous pouvez consulter la vidéo ici : https://www.youtube.com/watch?v=h02ti0Bl6zk
VIDEO_PATH = "data/keynote_recap.mp4"
```


```python
def process_video(video_path, seconds_per_frame=2):
    base64Frames = []
    base_video_path, _ = os.path.splitext(video_path)

    video = cv2.VideoCapture(video_path)
    total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))
    fps = video.get(cv2.CAP_PROP_FPS)
    frames_to_skip = int(fps * seconds_per_frame)
    curr_frame=0

    # Loop through the video and extract frames at specified sampling rate
    while curr_frame < total_frames - 1:
        video.set(cv2.CAP_PROP_POS_FRAMES, curr_frame)
        success, frame = video.read()
        if not success:
            break
        _, buffer = cv2.imencode(".jpg", frame)
        base64Frames.append(base64.b64encode(buffer).decode("utf-8"))
        curr_frame += frames_to_skip
    video.release()

    # Extract audio from video
    audio_path = f"{base_video_path}.mp3"
    clip = VideoFileClip(video_path)
    clip.audio.write_audiofile(audio_path, bitrate="32k")
    clip.audio.close()
    clip.close()

    print(f"Extracted {len(base64Frames)} frames")
    print(f"Extracted audio to {audio_path}")
    return base64Frames, audio_path

# Extract 1 frame per second. You can adjust the `seconds_per_frame` parameter to change the sampling rate
base64Frames, audio_path = process_video(VIDEO_PATH, seconds_per_frame=1)
```
**Introduction et aperçu de l'événement** :
- La vidéo commence avec le titre "OpenAI DevDay" et passe à "Récapitulatif de la conférence."
- Le lieu de l'événement est montré, avec des participants se rassemblant et la scène en cours de préparation.

**Présentation principale** :
- Un intervenant, vraisemblablement d'OpenAI, monte sur scène pour présenter.
- La présentation couvre divers sujets liés aux derniers développements et annonces d'OpenAI.

**Annonces** :
- **GPT-4 Turbo** : Introduction de GPT-4 Turbo, mettant en avant ses capacités et performances améliorées.
- **Mode JSON** : Une nouvelle fonctionnalité permettant la sortie de données structurées au format JSON.
- **Appel de fonctions** : Démonstration des capacités améliorées d'appel de fonctions, rendant les interactions plus efficaces.
- **Longueur de contexte et contrôle** : Améliorations de la longueur de contexte et du contrôle utilisateur sur les réponses du modèle.
- **Meilleure intégration des connaissances** : Améliorations de la base de connaissances et des capacités de récupération du modèle.

### Résumé audio
Le résumé audio est généré en envoyant au modèle la transcription audio. Avec seulement l'audio, le modèle est susceptible de biaiser vers le contenu audio et de manquer le contexte fourni par les présentations et les visuels.
L'entrée audio pour GPT-4o n'est actuellement pas disponible mais arrivera bientôt ! Pour l'instant, nous utilisons notre modèle existant whisper-1 pour traiter l'audio.

```python
# Transcrire l'audio
transcription = client.audio.transcriptions.create(
    model="whisper-1",
    file=open(audio_path, "rb"),
)
```
## Générer un résumé audio
```python
response = client.chat.completions.create(
    model=MODEL,
    messages=[
        {"role": "system", "content":"""Vous générez un résumé de transcription. Créez un résumé de la transcription fournie. Répondez en Markdown."""},
        {"role": "user", "content": [
            {"type": "text", "text": f"La transcription audio est : {transcription.text}"}
        ],
    ],
    temperature=0,
)
print(response.choices[0].message.content)
```
```
### Résumé :
Bienvenue à la toute première Dev Day d'OpenAI. … - **Nouvelles fonctionnalités** :
- **Dolly 3**, **GPT-4 Turbo avec vision**, et un nouveau **modèle de synthèse vocale** sont maintenant disponibles dans l'API.
- **Modèles personnalisés** : Un programme où les chercheurs d'OpenAI aident les entreprises à créer des modèles personnalisés adaptés à leurs cas d'utilisation spécifiques.
- **Augmentation des limites de taux** : Doublement des jetons par minute pour les clients GPT-4 établis et possibilité de demander des modifications supplémentaires des limites de taux.
- **GPTs** : Versions personnalisées de ChatGPT pour des objectifs spécifiques, programmables par conversation, avec des options de partage privé ou public, et une future boutique GPT.
- **API d'assistance** : Comprend des fils de discussion persistants, une récupération intégrée, un interprète de code et des appels de fonction améliorés. OpenAI est enthousiaste à propos de l'avenir de l'intégration de l'IA et se réjouit de voir ce que les utilisateurs créeront avec ces nouveaux outils. L'événement se termine par une invitation à revenir l'année prochaine pour plus d'avancées.
```
Le résumé audio est biaisé vers le contenu discuté pendant le discours, mais il est moins structuré que le résumé vidéo.

### Résumé audio + visuel
Le résumé audio + visuel est généré en envoyant au modèle à la fois le visuel et l'audio de la vidéo en une fois. En envoyant ces deux éléments, le modèle est censé mieux résumer car il peut percevoir la vidéo entière en même temps.

## Générer un résumé avec réponse visuelle et audio
```python
response = client.chat.completions.create(
    model=MODEL,
    messages=[
        {"role": "system", "content":"""Vous générez un résumé vidéo. Créez un résumé de la vidéo fournie et de sa transcription. Répondez en Markdown."""},
        {"role": "user", "content": [
            "Ce sont les cadres de la vidéo.",
            *map(lambda x: {"type": "image_url", "image_url": {"url": f'data:image/jpg;base64,{x}', "detail": "low"}}, base64Frames),
            {"type": "text", "text": f"La transcription audio est : {transcription.text}"}
        ],
    ],
    temperature=0,
)
print(response.choices[0].message.content)
```
```
### Résumé vidéo : OpenAI Dev Day
#### Introduction
- La vidéo commence par le titre "OpenAI Dev Day" et passe à un récapitulatif de la conférence.

#### Aperçu de l'événement
- L'événement se déroule dans un lieu avec une pancarte indiquant "OpenAI Dev Day."
- Les participants sont vus entrer et se rassembler dans une grande salle.

#### Présentation principale
- L'orateur principal introduit l'événement et annonce le lancement de GPT-4 Turbo.
- **GPT-4 Turbo** :
  - Supporte jusqu'à 128 000 jetons de contexte.
  - Introduit une nouvelle fonctionnalité appelée mode JSON pour des réponses JSON valides.
  - Capacités améliorées d'appel de fonctions.
  - Suivi des instructions amélioré et récupération des connaissances à partir de documents externes ou de bases de données.
  - Connaissances mises à jour jusqu'en avril 2023.
  - Disponible dans l'API avec DALL-E 3, GPT-4 Turbo avec vision, et un nouveau modèle de synthèse vocale.

#### Modèles personnalisés
- Lancement d'un nouveau programme appelé Modèles personnalisés.
- Les chercheurs collaboreront avec les entreprises pour créer des modèles personnalisés adaptés à des cas d'utilisation spécifiques.
- Limites de taux plus élevées et possibilité de demander des modifications des limites de taux et des quotas directement dans les paramètres de l'API.

#### Tarification et performance
- **GPT-4 Turbo** :
  - 3 fois moins cher pour les jetons de requête et 2 fois moins cher pour les jetons de réponse par rapport à GPT-4.
  - Doublement des jetons par minute pour les clients GPT-4 établis.
```

### 2 : Questions et réponses
Pour la session de questions-réponses, nous utiliserons le même concept qu'auparavant pour poser des questions sur notre vidéo traitée tout en exécutant les mêmes 3 tests pour démontrer l'avantage de combiner les modalités d'entrée :
- Questions-réponses visuelles
- Questions-réponses audio
- Questions-réponses visuelles + audio

QUESTION = "Question : Pourquoi Sam Altman a-t-il donné un exemple sur le fait de monter les fenêtres et d'allumer la radio ?"
```python
qa_visual_response = client.chat.completions.create(
    model=MODEL,
    messages=[
        {"role": "system", "content": "Utilisez la vidéo pour répondre à la question fournie. Répondez en Markdown."},
        {"role": "user", "content": [
            "Ce sont les cadres de la vidéo.",
            *map(lambda x: {"type": "image_url", "image_url": {"url": f'data:image/jpg;base64,{x}', "detail": "low"}}, base64Frames),
            QUESTION
        ],
    ],
    temperature=0,
)
print("QA visuelle :\n" + qa_visual_response.choices[0].message.content)
```
```
QA visuelle : Sam Altman a utilisé l'exemple sur le fait de monter les fenêtres et d'allumer la radio pour démontrer la capacité d'appel de fonctions de GPT-4 Turbo. L'exemple illustrait comment le modèle peut interpréter et exécuter plusieurs commandes de manière plus structurée et efficace. La comparaison "avant" et "après" a montré comment le modèle peut maintenant appeler directement des fonctions comme `raise_windows()` et `radio_on()` sur la base d'instructions en langage naturel, démontrant un contrôle et une fonctionnalité améliorés.
```

Combinons maintenant les cadres et l'audio.
```python
response = client.chat.completions.create(
    model=MODEL,
    messages=[
        {"role": "system", "content":"""Vous générez une réponse à une question basée sur la vidéo. Répondez en Markdown."""},
        {"role": "user", "content": [
            "Ce sont les cadres de la vidéo.",
            *map(lambda x: {"type": "image_url", "image_url": {"url": f'data:image/jpg;base64,{x}', "detail": "low"}}, base64Frames),
            {"type": "text", "text": f"La transcription audio est : {transcription.text}"},
            QUESTION
        ],
    ],
    temperature=0,
)
print(response.choices[0].message.content)
```
```
## Applications et cas d'utilisation réels de GPT-4o dans la summarisation vidéo

### Analyse vidéo en temps réel améliorée
GPT-4o introduit des avancées significatives dans l'analyse vidéo en temps réel, permettant une interprétation et une summarisation immédiates du contenu vidéo. Cette capacité est cruciale pour des applications telles que les diffusions d'actualités en direct et les événements sportifs où des résumés vidéo instantanés sont nécessaires pour l'engagement des spectateurs et la gestion du contenu. La capacité du modèle à analyser et résumer les flux vidéo en temps réel soutient les médias dans la diffusion de contenu pertinent et opportun à leur audience. Par exemple, lors d'un événement sportif en direct, GPT-4o peut générer instantanément des moments forts et des résumés, améliorant ainsi l'expérience de visionnage.

### Intégration de données multimodales pour des résumés complets
L'intégration de données textuelles, audio et visuelles par GPT-4o permet des résumés vidéo plus complets et contextuellement pertinents. Cette approche multimodale est particulièrement bénéfique dans le contenu éducatif, où les résumés vidéo peuvent inclure des descriptions textuelles, des points clés des conférences et des données visuelles pertinentes.

### Modération et conformité de contenu automatisées
Avec sa compréhension avancée du contenu vidéo, GPT-4o sert d'outil efficace pour la modération automatisée de contenu. Les plateformes hébergeant des vidéos générées par les utilisateurs peuvent utiliser GPT-4o pour analyser et scanner le contenu vidéo pour se conformer aux normes de sécurité numérique et aux directives communautaires. Cette application réduit le besoin de modérateurs humains et accélère le processus de modération de contenu, garantissant des environnements numériques plus sûrs. Par exemple, les plateformes de médias sociaux peuvent déployer GPT-4o pour détecter et gérer automatiquement les contenus impliquant des discours de haine, de la violence ou d'autres matériaux nuisibles, maintenant ainsi l'intégrité de la plateforme et la sécurité des utilisateurs.


```
Références :
[https://azure.microsoft.com/en-us/blog/introducing-gpt-4o-openais-new-flagship-multimodal-model-now-in-preview-on-azure/](https://azure.microsoft.com/en-us/blog/introducing-gpt-4o-openais-new-flagship-multimodal-model-now-in-preview-on-azure/)
